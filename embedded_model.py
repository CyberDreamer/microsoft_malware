import pandas as pd
import numpy as np
import lightgbm as lgb
#import xgboost as xgb
from scipy.sparse import vstack, csr_matrix, save_npz, load_npz
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.neural_network import MLPRegressor
#from sklearn.metrics import roc_auc_score
import gc
gc.enable()
from keras.models import Sequential, Model
from keras.layers import Dense, Embedding, Input, Reshape, Concatenate
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from keras import optimizers
from dtypes import dtypes

import time

print('load data')
N = 3000000
train_cat = np.load('train_cat_np_x.npy')[:]
train_num = np.load('train_num_np_x.npy')[:]
train_y = np.load('train_num_np_y.npy')[:]
test_cat = np.load('test_cat_np_x.npy')[:]
test_num = np.load('test_num_np_x.npy')[:]

# print(train_cat)
# print(train_num)
# print(train_y)
# exit()

inputs = []
embeddings = []
train_x = []
test_x = []


print('build net')
for i in range(train_cat.shape[1]): 
    categories_count = train_cat[:,i].max()
    # print(categories_count)

    vector_size = 3
    
    if categories_count > 10 and categories_count <= 20:
        vector_size = 10
    if categories_count > 20 and categories_count <= 100:
        vector_size = 20
    if categories_count > 100 and categories_count <= 1000:
        vector_size = 60
    if categories_count > 1000:
        vector_size = 100

    input_layer = Input(shape=(1,))
    embedding = Embedding(categories_count, vector_size, input_length=1)(input_layer)
    embedding = Reshape(target_shape=(vector_size,))(embedding)

    if categories_count > 20:
        embedding = Dense(16, activation='relu')(embedding)
    # if categories_count > 1000:
    #     embedding = Dense(32, activation='relu')(embedding)

    inputs.append(input_layer)
    embeddings.append(embedding)
    train_x.append(train_cat[:, i])
    test_x.append(test_cat[:, i])

input_layer = Input(shape=(train_num.shape[1],))
embedding = Dense(16, activation='relu')(input_layer)
inputs.append(input_layer)
embeddings.append(embedding)
train_x.append(train_num)
# NUMERIC
x = Concatenate()(embeddings)
# x = Dense(16, activation='relu')(x)
# x = Dense(8, activation='relu')(x)
x = Dense(4, activation='relu')(x)
output = Dense(1, activation='sigmoid')(x)
model = Model(inputs, output)
optimizer = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

test_x.append(test_num)

train_count = 8921483
test_count = 7853253
# N = 2973827
episodes = 12
N = int(train_count / episodes)
M = 2 * N
print('step = ', N)
results = np.zeros(test_count)
for episode in range(episodes):
    print('start episode - {}/{}'.format(episode+1, episodes))
    print('load numpy arrays')
    print(time.gmtime())
    train_x = np.load('train_x_labeled.np.npy')
    train_y = np.load('train_ynp.npy')
    # print(train_x.shape)
    print(train_x[2,:])
    exit()

    # idx = np.random.randint(train_count, size=M)
    # train_x_part = train_x[idx,:]
    # train_y_part = train_y[idx]

    start = N*episode
    end = start + N
    train_x_part = train_x[start:end,:]
    train_y_part = train_y[start:end]

    del train_x, train_y
    gc.collect()

    # import pickle
    print('start training')
    print(time.gmtime())
    model.fit(train_x_part, train_y_part, epochs=10, batch_size=128)

    # pickle.dump(clf, open('random_forest.mdl', 'wb'))
    # clf = pickle.load(open('random_forest.mdl', 'rb'))

    del train_x_part, train_y_part
    gc.collect()


    test_x = np.load('test_x_labeled.np.npy')
    fkck = np.zeros((test_x.shape[0], 1))
    test_x = np.hstack((fkck, test_x))

    print('start prediction')
    print(time.gmtime())
    results += model.predict(test_x, batch_size=128)[:,0]
    # results += model.predict(test_x)
    print(results.shape)
    np.save('results', results)
    # accuracy = roc_auc_score(train_y, predictions)
    # print('accuracy: ', accuracy)

    del test_x
    # del model
    gc.collect()


submission = pd.read_csv('sample_submission.csv')
submission['HasDetections'] = results/ episodes
submission.to_csv('tree_label_submission.csv', index=False)

print('\nDone.')